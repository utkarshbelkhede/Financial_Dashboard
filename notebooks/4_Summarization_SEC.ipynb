{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkarshbelkhede/Financial_Dashboard/blob/master/notebooks/4_Summarization_SEC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Installing Required Libraries"
      ],
      "metadata": {
        "id": "Eae-KPeDcnVA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers rouge-score nltk huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install git-lfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPAANvM25e4T",
        "outputId": "3d6fac82-00a4-4e8a-c258-01b5b77c7f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Loading Required Libraries"
      ],
      "metadata": {
        "id": "sOm7WGRCcuMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from datasets import load_dataset, load_metric\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBEU7CmCSfMp",
        "outputId": "f537ad93-04cd-477e-c60f-55e36fbf828d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Config"
      ],
      "metadata": {
        "id": "dSo76T3Fc4AP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN = \"\"\n",
        "\n",
        "MODEL_CHECKPOINT = \"t5-small\"\n",
        "\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "\n",
        "MAX_TARGET_LENGTH = 128\n",
        "\n",
        "CSV_FILE = \"flat.csv\"\n",
        "\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "MODEL_NAME = MODEL_CHECKPOINT.split(\"/\")[-1] + \"-\" + CSV_FILE.split('/')[-1].split('.')[-2]\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "DECAY = 0.01\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "PUSH_TO_HUB = True\n",
        "\n",
        "FP16 = False"
      ],
      "metadata": {
        "id": "tYPgPWSk585i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Functions"
      ],
      "metadata": {
        "id": "LFKE9Laec_VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(FILE, TRAIN_SIZE):\n",
        "  try:\n",
        "    data = pd.read_csv(FILE, na_values=' ')\n",
        "  except:\n",
        "    print(f\"Cannot Open {FILE}.\")\n",
        "    return None\n",
        "  else:\n",
        "    data.fillna(\"-\", inplace=True)\n",
        "\n",
        "    raw_datasets = Dataset.from_pandas(data)\n",
        "\n",
        "    raw_datasets = raw_datasets.train_test_split(train_size=TRAIN_SIZE)\n",
        "\n",
        "    return raw_datasets"
      ],
      "metadata": {
        "id": "wz3tWM5xd1Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, model_max_length=MAX_INPUT_LENGTH)\n",
        "\n",
        "  prefix = \"summarize: \"\n",
        "\n",
        "  inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "  model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
        "\n",
        "  # Setup the tokenizer for targets\n",
        "  labels = tokenizer(text_target=examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
        "\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    \n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "zg1cKfL9hPUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "\n",
        "  # Loading Rouge Metric\n",
        "  metric = load_metric(\"rouge\")\n",
        "  print(\"Loaded Rouge Metric\")\n",
        "\n",
        "  predictions, labels = eval_pred\n",
        "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "  # Replace -100 in the labels as we can't decode them.\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "  # Rouge expects a newline after each sentence\n",
        "  decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "  decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "  result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "  # Extract a few results\n",
        "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "  # Add mean generated length\n",
        "  prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "  return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "xFi3T09mmM4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def huggingface_summarization():  \n",
        "  try:\n",
        "    if PUSH_TO_HUB:\n",
        "      login(token=TOKEN)\n",
        "  except:\n",
        "    print(\"Invalid Huggingface Access Token\")\n",
        "  else:\n",
        "    # Loading csv and Splitting that data\n",
        "    raw_datasets = create_dataset(CSV_FILE, TRAIN_SIZE)\n",
        "\n",
        "    if raw_datasets != None:\n",
        "      print(f\"Loaded {CSV_FILE} Successfully(with Split Train Size {TRAIN_SIZE})\")\n",
        "\n",
        "      # Loading AutoTokenizer\n",
        "      tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, model_max_length=MAX_INPUT_LENGTH)\n",
        "      print(f\"Loaded AutoTokenizer for {MODEL_CHECKPOINT} model\")\n",
        "\n",
        "      # Tokenize Data\n",
        "      tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "      print(\"Tokenized Data Successfully\")\n",
        "\n",
        "      # Loading Model\n",
        "      model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "      print(f\"Loaded {MODEL_CHECKPOINT} successfully\")\n",
        "\n",
        "      # Loading Data Collator\n",
        "      data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "      args = Seq2SeqTrainingArguments(\n",
        "        MODEL_NAME,\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        learning_rate = LEARNING_RATE,\n",
        "        per_device_train_batch_size = BATCH_SIZE,\n",
        "        per_device_eval_batch_size = BATCH_SIZE,\n",
        "        weight_decay = DECAY,\n",
        "        save_total_limit = 3,\n",
        "        num_train_epochs = EPOCHS,\n",
        "        predict_with_generate = True,\n",
        "        fp16 = FP16,\n",
        "        push_to_hub = PUSH_TO_HUB,\n",
        "      )\n",
        "\n",
        "      trainer = Seq2SeqTrainer(\n",
        "        model,\n",
        "        args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"test\"],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "      )\n",
        "\n",
        "      # Training Model\n",
        "      trainer.train()\n",
        "      print(\"Trained Model Successfully\")\n",
        "\n",
        "      if PUSH_TO_HUB:\n",
        "        trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "2IwhG5mMTrn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. FineTuning Call"
      ],
      "metadata": {
        "id": "D7Gz4pbwdGoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "huggingface_summarization()"
      ],
      "metadata": {
        "id": "1xEW7Tt_ekeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Testing"
      ],
      "metadata": {
        "id": "CHKsegqsdQp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4KX_aF4nNgj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"utkarshbelkhede/t5-small-sec\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Weavy2Z76ak0"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkLoxGAv4les"
      },
      "outputs": [],
      "source": [
        "summary = pipeline(model=model_name, tokenizer=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu9hj0k36sHH"
      },
      "outputs": [],
      "source": [
        "summary(data.iloc[2,0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QuFIwPagN2_7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}